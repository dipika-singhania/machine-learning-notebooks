{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>64.788465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1241.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Votes\n",
       "count  2150.000000\n",
       "mean     24.011628\n",
       "std      64.788465\n",
       "min       2.000000\n",
       "25%       4.000000\n",
       "50%       8.000000\n",
       "75%      19.000000\n",
       "max    1241.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_csv('voted-kaggle-dataset.csv')\n",
    "file.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Title  \\\n",
      "0  Credit Card Fraud Detection   \n",
      "1     European Soccer Database   \n",
      "2      TMDB 5000 Movie Dataset   \n",
      "3    Global Terrorism Database   \n",
      "4      Bitcoin Historical Data   \n",
      "\n",
      "                                            Subtitle  \\\n",
      "0  Anonymized credit card transactions labeled as...   \n",
      "1  25k+ matches, players & teams attributes for E...   \n",
      "2                Metadata on ~5,000 movies from TMDb   \n",
      "3  More than 170,000 terrorist attacks worldwide,...   \n",
      "4  Bitcoin data at 1-min intervals from select ex...   \n",
      "\n",
      "                          Owner  Votes  \\\n",
      "0  Machine Learning Group - ULB   1241   \n",
      "1                  Hugo Mathien   1046   \n",
      "2     The Movie Database (TMDb)   1024   \n",
      "3              START Consortium    789   \n",
      "4                        Zielak    618   \n",
      "\n",
      "                                            Versions  \\\n",
      "0          Version 2,2016-11-05|Version 1,2016-11-03   \n",
      "1  Version 10,2016-10-24|Version 9,2016-10-24|Ver...   \n",
      "2                               Version 2,2017-09-28   \n",
      "3          Version 2,2017-07-19|Version 1,2016-12-08   \n",
      "4  Version 11,2018-01-11|Version 10,2017-11-17|Ve...   \n",
      "\n",
      "                                        Tags Data Type    Size License  \\\n",
      "0                             crime\\nfinance       CSV  144 MB    ODbL   \n",
      "1               association football\\neurope    SQLite  299 MB    ODbL   \n",
      "2                                       film       CSV   44 MB   Other   \n",
      "3  crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
      "4                           history\\nfinance       CSV  119 MB     CC4   \n",
      "\n",
      "           Views          Download        Kernels     Topics  \\\n",
      "0  442,136 views  53,128 downloads  1,782 kernels  26 topics   \n",
      "1  396,214 views  46,367 downloads  1,459 kernels  75 topics   \n",
      "2  446,255 views  62,002 downloads  1,394 kernels  46 topics   \n",
      "3  187,877 views  26,309 downloads    608 kernels  11 topics   \n",
      "4  146,734 views  16,868 downloads     68 kernels  13 topics   \n",
      "\n",
      "                                                 URL  \\\n",
      "0     https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
      "1          https://www.kaggle.com/hugomathien/soccer   \n",
      "2    https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
      "3               https://www.kaggle.com/START-UMD/gtd   \n",
      "4  https://www.kaggle.com/mczielinski/bitcoin-his...   \n",
      "\n",
      "                                         Description  \n",
      "0  The datasets contains transactions made by cre...  \n",
      "1  The ultimate Soccer database for data analysis...  \n",
      "2  Background\\nWhat can we say about the success ...  \n",
      "3  Context\\nInformation on more than 170,000 Terr...  \n",
      "4  Context\\nBitcoin is the longest running and mo...  \n"
     ]
    }
   ],
   "source": [
    "print(file.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizing 1 sentence\n",
      "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
      "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
      "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
      "Tokenized Words ['The', 'datasets', 'contains', 'transactions', 'made', 'by', 'credit', 'cards', 'in', 'September', '2013', 'by', 'european', 'cardholders.', 'This', 'dataset', 'presents', 'transactions', 'that', 'occurred', 'in', 'two', 'days', ',', 'where', 'we', 'have', '492', 'frauds', 'out', 'of', '284,807', 'transactions.', 'The', 'dataset', 'is', 'highly', 'unbalanced', ',', 'the', 'positive', 'class', '(', 'frauds', ')', 'account', 'for', '0.172', '%', 'of', 'all', 'transactions.', 'It', 'contains', 'only', 'numerical', 'input', 'variables', 'which', 'are', 'the', 'result', 'of', 'a', 'PCA', 'transformation.', 'Unfortunately', ',', 'due', 'to', 'confidentiality', 'issues', ',', 'we', 'can', 'not', 'provide', 'the', 'original', 'features', 'and', 'more', 'background', 'information', 'about', 'the', 'data.', 'Features', 'V1', ',', 'V2', ',', '...', 'V28', 'are', 'the', 'principal', 'components', 'obtained', 'with', 'PCA', ',', 'the', 'only', 'features', 'which', 'have', 'not', 'been', 'transformed', 'with', 'PCA', 'are', \"'Time\", \"'\", 'and', \"'Amount'.\", 'Feature', \"'Time\", \"'\", 'contains', 'the', 'seconds', 'elapsed', 'between', 'each', 'transaction', 'and', 'the', 'first', 'transaction', 'in', 'the', 'dataset.', 'The', 'feature', \"'Amount\", \"'\", 'is', 'the', 'transaction', 'Amount', ',', 'this', 'feature', 'can', 'be', 'used', 'for', 'example-dependant', 'cost-senstive', 'learning.', 'Feature', \"'Class\", \"'\", 'is', 'the', 'response', 'variable', 'and', 'it', 'takes', 'value', '1', 'in', 'case', 'of', 'fraud', 'and', '0', 'otherwise.', 'Given', 'the', 'class', 'imbalance', 'ratio', ',', 'we', 'recommend', 'measuring', 'the', 'accuracy', 'using', 'the', 'Area', 'Under', 'the', 'Precision-Recall', 'Curve', '(', 'AUPRC', ')', '.', 'Confusion', 'matrix', 'accuracy', 'is', 'not', 'meaningful', 'for', 'unbalanced', 'classification.', 'The', 'dataset', 'has', 'been', 'collected', 'and', 'analysed', 'during', 'a', 'research', 'collaboration', 'of', 'Worldline', 'and', 'the', 'Machine', 'Learning', 'Group', '(', 'http', ':', '//mlg.ulb.ac.be', ')', 'of', 'ULB', '(', 'Université', 'Libre', 'de', 'Bruxelles', ')', 'on', 'big', 'data', 'mining', 'and', 'fraud', 'detection.', 'More', 'details', 'on', 'current', 'and', 'past', 'projects', 'on', 'related', 'topics', 'are', 'available', 'on', 'http', ':', '//mlg.ulb.ac.be/BruFence', 'and', 'http', ':', '//mlg.ulb.ac.be/ARTML', 'Please', 'cite', ':', 'Andrea', 'Dal', 'Pozzolo', ',', 'Olivier', 'Caelen', ',', 'Reid', 'A.', 'Johnson', 'and', 'Gianluca', 'Bontempi.', 'Calibrating', 'Probability', 'with', 'Undersampling', 'for', 'Unbalanced', 'Classification.', 'In', 'Symposium', 'on', 'Computational', 'Intelligence', 'and', 'Data', 'Mining', '(', 'CIDM', ')', ',', 'IEEE', ',', '2015']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "print('Analizing 1 sentence')\n",
    "print(file.loc[0]['Description'])\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenized_words = tokenizer.tokenize(file.loc[0]['Description'])\n",
    "print('Tokenized Words', tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def find_pos(word):\n",
    "    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags -'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags -'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags -'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def words_lemmatizer(text, encoding = 'utf-8'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl= WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos= find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos))\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    datasets contains transaction make credit card...\n",
      "1    ultimate soccer database data analysis machine...\n",
      "2    background say success movie release certain c...\n",
      "3    context information 170 000 terrorist attack g...\n",
      "4    context bitcoin long run well know cryptocurre...\n",
      "Name: Description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    "\n",
    "# Converting to lower case\n",
    "def do_prepocessing(one_row):\n",
    "    try:\n",
    "        lower_text = one_row.lower()\n",
    "        \n",
    "        remove_unwanted_charectors = re.sub(r'[^a-zA-Z0-9_.:\\s]', ' ', lower_text) # Remove unwated charectors like punctuations andnon ascii \n",
    "        remove_unwanted_charectors = re.sub(r'&[\\w]+', ' ', remove_unwanted_charectors) # Remove &amp, *&words etc\n",
    "\n",
    "        removed_extra_space = re.sub(r'\\s+',' ', remove_unwanted_charectors) # Remove extra white_spaces\n",
    "\n",
    "        removed_stopwords_text = remove_stopwords(removed_extra_space)\n",
    "        lemmatize_text = words_lemmatizer(removed_stopwords_text)\n",
    "        #print(\"count=\", count, \"# text = \", extract_has_joined, \"Actual text \",lemmatize_text )\n",
    "        return lemmatize_text\n",
    "    except:\n",
    "        return one_row\n",
    "\n",
    "\n",
    "new_df = file['Description'].apply(do_prepocessing)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dropna(inplace=True, axis=0)\n",
    "sum(new_df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=7, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF,LatentDirichletAllocation\n",
    "nmf_model = NMF(n_components=7,random_state=42)\n",
    "nmf_model.fit(tfidf.transform(new_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=7, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "LDA.fit(tfidf.transform(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31742"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['contains', 'service', 'airbnb', 'subset', 'house', 'extract', 'hotel', 'big', 'promptcloud', 'create', 'review', 'job', 'crawl', 'dataset', 'description']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['contains', 'time', 'include', 'set', 'number', 'state', 'information', 'city', 'price', 'file', 'use', 'year', 'csv', 'dataset', 'data']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['science', 'easy', 'thanks', 'past', 'community', 'citation', 'story', 'question', 'opportunity', 'acquire', 'attribution', 'answer', 'inside', 'data', 'owe']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['imagenet', 'deep', 'convolutional', 'architecture', 'transferable', 'depth', 'residual', 'layer', 'network', 'learn', 'feature', 'image', 'pre', 'model', 'train']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['sport', 'csv', 'win', 'stats', 'com', 'football', 'play', 'data', 'league', 'score', 'season', 'match', 'team', 'game', 'player']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['id', 'article', 'file', 'user', 'sentiment', 'speech', 'dataset', 'english', 'vector', 'use', 'tweet', 'corpus', 'text', 'language', 'word']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['simoiu', 'overgoor', 'law', 'forfeiture', 'officer', 'enforcement', 'project', 'united', 'open', 'jurisdiction', 'state', 'stanford', 'traffic', 'stop', 'police']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# words in NMF mdoeling\n",
    "\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['simoiu', 'ramachandran', 'overgoor', 'corbett', 'anime', 'song', 'fivethirtyeight', 'officer', 'scorecard', 'reliably', 'police', 'stop', 'coco', 'jurisdiction', 'pesticide']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['anime', 'powerball', 'wikihow', 'postal', 'en', 'le', 'minneapolis', 'nip', 'pollutant', 'zipcode', 'starbucks', 'epa', 'el', 'airbnb', 'openaddresses']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['terror', 'crash', 'dry', 'voyant', 'pollster', 'marathon', 'slave', 'yelp', 'msa', 'gun', 'specie', 'shooting', 'tatoeba', 'pill', 'loan']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['pharmacy', 'properati', 'liver', 'meteorite', 'homicide', 'song', 'murder', 'pump', 'victim', 'specie', 'ted', 'breach', 'uber', 'dataset', 'description']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['plaque', 'toronto', 'nypd', 'podcast', 'glacier', 'densenet', 'marijuana', 'ufo', 'metacritic', 'squeezenet', 'bike', 'volcano', 'emoji', 'specie', 'thor']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['image', 'information', 'acknowledgement', 'year', 'contains', 'csv', 'context', 'http', 'include', 'content', 'time', 'file', 'use', 'dataset', 'data']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['boardgamegeek', 'assault', 'officer', 'ucla', 'baltimore', 'pm2', 'quora', 'murder', '911', 'marathon', 'fbi', 'seattle', 'spotify', 'inmate', 'song']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# words for LDA modeling\n",
    "for index, topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = nmf_model.transform(tfidf.transform(new_df)).argmax(axis=1)\n",
    "topic_results_lda = LDA.transform(tfidf.transform(new_df)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    datasets contains transaction make credit card...\n",
       "1    ultimate soccer database data analysis machine...\n",
       "2    background say success movie release certain c...\n",
       "3    context information 170 000 terrorist attack g...\n",
       "4    context bitcoin long run well know cryptocurre...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_lda[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the most common words will help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
